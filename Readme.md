# ğŸš€ Faster R-CNN w/ Kalman Filter API (FastAPI + PyTorch)

A lightweight web API for **real-time object detection** using a trained **Faster R-CNN with Kalman Filter** model.  
Includes endpoints for image prediction, visualization, and live camera streaming â€” optimized for smooth performance on both **macOS (M-series)** and **Windows**.

---

## ğŸ§  Features
- **REST API** built with FastAPI  
- **Image upload** detection endpoint (`/predict-image`)  
- **Visualization endpoint** returning annotated images (`/visualize-image`)  
- **Live camera streaming** (`/live`) with threaded inference  
- **Automatic saving** of:
  - ğŸ–¼ï¸ Predicted images (`/results/images`)
  - ğŸ“„ Detection JSON (`/results/json`)
  - ğŸ“¦ COCO-format annotations (`/results/coco`)
- Compatible with **CPU**, **CUDA**, and **Apple Silicon (MPS)**  
- Fully **Docker-ready** and works with or without attached storage volumes  

---

## ğŸ§  Trained Model
ğŸ“¦ Model weights (Faster R-CNN + Kalman Filter)  
ğŸ”— [Request Access Here](https://drive.google.com/file/d/1KC9LZ1u8av3O4lO-_VJ8r9P_2PHnzsLU/view?usp=drive_link)

---

## ğŸ–¼ï¸ System Sample

Hosted via [LocalTunnel](https://theboroer.github.io/localtunnel-www/).  
LocalTunnel uses your **local IP address** as a one-time password (see credentials in the shared Drive `.txt` file).

### ğŸ”¹ Image / Video Analysis  
ğŸ”— [https://gd-live.loca.lt/](https://gd-live.loca.lt/)

<p align="center">
  <img src="/assets/1.png" alt="System Sample" width="600"/>
</p>

### ğŸ”¹ Live Detection  
ğŸ”— [https://gd-live.loca.lt/live](https://gd-live.loca.lt/live)

<p align="center">
  <img src="/assets/2.png" alt="System Sample" width="600"/>
</p>

### ğŸ”¹ API Docs  
ğŸ”— [https://gd-live.loca.lt/docs](https://gd-live.loca.lt/docs)

<p align="center">
  <img src="/assets/3.png" alt="System Sample" width="600"/>
</p>

---

## ğŸ“¦ Requirements

Python 3.10+

### Dependencies
```bash
fastapi==0.115.0
uvicorn[standard]==0.30.6
torch==2.4.1
torchvision==0.19.1
pillow==10.4.0
pydantic==1.10.13
numpy==1.26.4
python-multipart==0.0.9
opencv-python==4.10.0.84
```

---

## âš™ï¸ Installation (Local â€“ Mac / Windows)

```bash
# 1ï¸âƒ£ Clone the repository
git clone https://github.com/fglend/kalman-fastercnn.git
cd kalman-fastercnn

# 2ï¸âƒ£ Create a virtual environment
python -m venv venv
venv\Scripts\activate       # Windows PowerShell  
# or
source venv/bin/activate    # macOS / Linux

# 3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

# 4ï¸âƒ£ Run the API locally
uvicorn app.main:app --host 0.0.0.0 --port 8080 --reload
```

Then open:
- API Docs â†’ [http://localhost:8080/docs](http://localhost:8080/docs)
- Live Stream â†’ [http://localhost:8080/live](http://localhost:8080/live)

---

## ğŸ³ Docker Deployment

```bash
# Build image
docker build -t fasterrcnn-api .
```

### ğŸ§© Run the Container
```bash
# Simple run
docker run -p 8080:8080 fasterrcnn-api
```

### ğŸ’¾ Mount your model or results folder
```bash
# macOS / Linux
docker run -p 8080:8080 -v $(pwd)/models:/models -v $(pwd)/results:/results fasterrcnn-api

# Windows PowerShell
docker run -p 8080:8080 -v ${PWD}/models:/models -v ${PWD}/results:/results fasterrcnn-api
```

> The container will automatically detect `/models` and `/results`.  
> If no volume is attached, it uses Dockerâ€™s internal storage (ephemeral).

---

## ğŸ§© Auto-Saving Behavior

Whenever `/predict-image` or `/visualize-image` is called, the app automatically:
1. Saves the **uploaded image** to `/results/images/`
2. Writes a **detection JSON** file to `/results/json/`
3. Generates a **COCO-format annotation** file to `/results/coco/`

### Example:
```
âœ… Saved: /results/images/20251027_031154.jpg  
âœ… Saved: /results/json/20251027_031154.json  
âœ… Saved: /results/coco/20251027_031154.json
```

---

## ğŸ” API Endpoints

### **1ï¸âƒ£ Health Check**
**GET** `/health`  
Returns the modelâ€™s current device and runtime info.

---

### **2ï¸âƒ£ Predict Image**
**POST** `/predict-image`  
Uploads an image and returns detections in JSON format.  
Also saves predictions automatically.

```bash
curl -X POST "http://localhost:8080/predict-image" -F "file=@sample.jpg"
```

---

### **3ï¸âƒ£ Visualize Image**
**POST** `/visualize-image`  
Returns an annotated image (JPEG) showing all detections.

```bash
curl -X POST "http://localhost:8080/visualize-image" -F "file=@sample.jpg" --output output.jpg
```

---

### **4ï¸âƒ£ Live Stream**
**GET** `/live`  
Starts live camera detection (works on Chrome desktop or mobile devices on the same network).

---

## âš™ï¸ Configuration

Edit `app/config.py` for environment and threshold setup:
```python
DEVICE = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
NUM_THREADS = 4
MODEL_PATH = "models/best_model.pth"
SCORE_THRESH = 0.5
```

---

## âš¡ Performance Tips
- ğŸ§® Use **CUDA (NVIDIA)** or **MPS (Mac)** for faster inference  
- ğŸ–¼ï¸ Lower `T.Resize()` in preprocessing for better FPS  
- âš™ï¸ Adjust frame skip in `generate_frames()` to balance accuracy vs speed  
- ğŸš€ Close unused browser tabs while streaming to improve latency  

---

## ğŸ§  Notes for macOS Users
If you see:
```
[ WARN:0] VIDEOIO(V4L2:/dev/video0): can't open camera by index
```
Try changing:
```python
cap = cv2.VideoCapture(1)
```
Or enable camera permissions:
> **System Settings â†’ Privacy & Security â†’ Camera â†’ Allow Terminal / IDE**

---

## ğŸ“ Project Structure
```
.
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI app
â”‚   â”œâ”€â”€ model.py             # Model loader
â”‚   â”œâ”€â”€ predict_utils.py     # Preprocessing & filtering
â”‚   â”œâ”€â”€ config.py            # Environment configuration
â”‚   â””â”€â”€ templates/           # HTML UI templates
â”œâ”€â”€ models/
â”‚   â””â”€â”€ best_model.pth       # Trained weights
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ images/              # Saved predictions
â”‚   â”œâ”€â”€ json/                # Detection outputs
â”‚   â””â”€â”€ coco/                # COCO annotations
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â””â”€â”€ README.md
```

---

## ğŸ‘¨â€ğŸ’» Author
**Glend Dale Ferrer**  
ğŸ“§ mgdferrer@tip.edu.ph  

---

## ğŸ“œ License
MIT License Â© 2025 Glend Dale Ferrer
